{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these libraries for the re-implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data.data import RotatedMNIST\n",
    "from modules.modules import Encoder, Classifier, Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available')\n",
    "else:\n",
    "    print('CUDA is available')\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parameters for the experiment\"\"\"\n",
    "# build the network\n",
    "input_dim = 28\n",
    "hidden_dim = 256\n",
    "d_hidden_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "num_features = 100\n",
    "num_classes = 10\n",
    "domain_dim = 1\n",
    "\n",
    "# train the network\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "lr = 2e-4\n",
    "weight_decay = 5e-4\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "gamma = 0.5 ** (1 / 50)\n",
    "lambda_gan = 2.0\n",
    "lr_scheduler = lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RotatedMNIST(rotation_range=(0, 360))\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=num_features,\n",
    "    dropout=dropout,\n",
    "    domain_dim=domain_dim\n",
    ")\n",
    "\n",
    "classifier = Classifier(\n",
    "    input_dim=num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=num_classes\n",
    ")\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    input_dim=num_features,\n",
    "    hidden_dim=d_hidden_dim,\n",
    "    output_dim=domain_dim\n",
    ")\n",
    "\n",
    "if train_on_gpu:\n",
    "    encoder = encoder.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for CIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "def to_tensor(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "    else:\n",
    "        x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def init_weight(self, net=None):\n",
    "    if net is None:\n",
    "        net = self\n",
    "    for m in net.modules():\n",
    "        # if the layer is a Linear layer, then\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0, std=0.01) # fills the input weight with values drawn from the normal distribution\n",
    "            nn.init.constant_(m.bias, val=0) # fills the input bias with the value 0\n",
    "\n",
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "def acc_reset_mnist():\n",
    "    hit_domain, cnt_domain = np.zeros(8), np.zeros(8)\n",
    "    acc_source, acc_target = 0, 0\n",
    "    cnt_source, cnt_target = 0, 0\n",
    "    hit_source, hit_target = 0, 0\n",
    "    \n",
    "    return hit_domain, cnt_domain, acc_source, acc_target, cnt_source, cnt_target, hit_source, hit_target\n",
    "\n",
    "def acc_update_mnist(y, g, domain, is_source, metrics_domain, metrics_source, metrics_target):\n",
    "    Y = to_np(y)\n",
    "    G = to_np(g)\n",
    "    T = to_np(domain)\n",
    "    T = (T * 8).astype(np.int32)\n",
    "    T[T >= 8] = 7\n",
    "    hit = (Y == G).astype(np.float32)\n",
    "\n",
    "    is_s = to_np(is_source)\n",
    "\n",
    "    hit_domain, cnt_domain = metrics_domain\n",
    "    acc_source, cnt_source, hit_source = metrics_source\n",
    "    acc_target, cnt_target, hit_target = metrics_target\n",
    "\n",
    "    for i in range(8):\n",
    "        hit_domain[i] += hit[T == i].sum()\n",
    "        cnt_domain[i] += (T == i).sum()\n",
    "    acc_domain = hit_domain / (cnt_domain + 1e-10)\n",
    "    acc_source, acc_target = acc_domain[0], acc_domain[1:].mean()\n",
    "    acc_domain = np.round(acc_domain, decimals=3)\n",
    "    acc_source = np.round(acc_source, decimals=3)\n",
    "    acc_target = np.round(acc_target, decimals=3)\n",
    "\n",
    "    cnt_source += (is_s == 1).sum()\n",
    "    cnt_target += (is_s == 0).sum()\n",
    "\n",
    "    hit_source += (hit[is_s == 1]).sum()\n",
    "    hit_target += (hit[is_s == 0]).sum()\n",
    "\n",
    "    return hit_domain, cnt_domain, acc_source, acc_target, cnt_source, cnt_target, hit_source, hit_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuously Indexed Domain Adaptation (CIDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, classifier, discriminator, epochs, lr_scheduler):\n",
    "    init_weight(encoder)\n",
    "\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    d_criterion = nn.MSELoss()\n",
    "    optimizer = Adam(\n",
    "        list(encoder.parameters()) + list(classifier.parameters()),\n",
    "        lr=lr,\n",
    "        betas=(beta1, beta2),\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    d_optimizer = Adam(\n",
    "        discriminator.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(beta1, beta2),\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    e_lr_scheduler = lr_scheduler.ExponentialLR(\n",
    "        optimizer=optimizer,\n",
    "        gamma=gamma\n",
    "    )\n",
    "    d_lr_scheduler = lr_scheduler.ExponentialLR(\n",
    "        optimizer=d_optimizer,\n",
    "        gamma=gamma\n",
    "    )\n",
    "    lr_schedulers = [e_lr_scheduler, d_lr_scheduler]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        accuracies = []\n",
    "        d_losses = []\n",
    "        e_gan_losses = []\n",
    "        e_pred_losses = []\n",
    "        hit_domain, cnt_domain, acc_source, acc_target, cnt_source, cnt_target, hit_source, hit_target = acc_reset_mnist()\n",
    "\n",
    "        for batch_idx, (x, y, u, domain) in enumerate(train_dataloader):\n",
    "            x, y, u, domain = x.to(device), y.to(device), u.to(device), domain.to(device)\n",
    "            domain = domain[:, 0]\n",
    "            is_source = (domain < 1.0 / 8).to(torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            x_align, features = encoder(x, u)\n",
    "            predictions = classifier(features)\n",
    "\n",
    "            set_requires_grad(discriminator, True)\n",
    "            d = discriminator(features.detach())\n",
    "            d_src = d_criterion(d[is_source == 1], u[is_source == 1])\n",
    "            d_tgt = d_criterion(d[is_source == 0], u[is_source == 0])\n",
    "            d_loss = (d_src + d_tgt) / 2\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            set_requires_grad(discriminator, False)\n",
    "            d = discriminator(features)\n",
    "            e_gan_src = d_criterion(d[is_source == 1], u[is_source == 1])\n",
    "            e_gan_tgt = d_criterion(d[is_source == 0], u[is_source == 0])\n",
    "            e_gan_loss = (e_gan_src + e_gan_tgt) / 2\n",
    "\n",
    "            y_src = y[is_source == 1]\n",
    "            predictions_src = predictions[is_source == 1]\n",
    "            e_pred_loss = criterion(predictions_src, y_src)\n",
    "            e_loss = e_gan_loss * lambda_gan + e_pred_loss\n",
    "            e_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            d_losses.append(d_loss.detach().item())\n",
    "            e_gan_losses.append(e_gan_loss.detach().item())\n",
    "            e_pred_losses.append(e_pred_loss.detach().item())\n",
    "\n",
    "            g = torch.argmax(predictions.detach(), dim=1)\n",
    "            metrics_domain = hit_domain, cnt_domain\n",
    "            metrics_source = acc_source, cnt_source, hit_source\n",
    "            metrics_target = acc_target, cnt_target, hit_target\n",
    "            hit_domain, cnt_domain, acc_source, acc_target, cnt_source, cnt_target, hit_source, hit_target = acc_update_mnist(y, g, domain, is_source, metrics_domain, metrics_source, metrics_target)    \n",
    "\n",
    "        # print for each epoch\n",
    "        print(f'Epoch: {epoch + 1} \\n \\t D Loss:{torch.tensor(d_losses).mean():0.3f} \\t E_gan Loss:{torch.tensor(e_gan_losses).mean():0.3f} \\t E_pred Loss:{torch.tensor(e_pred_losses).mean():0.3f}')\n",
    "        print(f' \\t Source Acc:{acc_source:0.3f} ({hit_source}/{cnt_source}) \\t Target Acc:{acc_target:0.3f} ({hit_target}/{cnt_target})')\n",
    "        test(encoder, classifier, discriminator)\n",
    "\n",
    "        for lr_scheduler in lr_schedulers:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return encoder, classifier, discriminator\n",
    "\n",
    "def test(encoder, classifier, discriminator):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    hit_domain, cnt_domain, acc_source, acc_target, cnt_source, cnt_target, hit_source, hit_target = acc_reset_mnist()\n",
    "\n",
    "    for batch_idx, (x, y, u, domain) in enumerate(test_dataloader):\n",
    "        x, y, u, domain = x.to(device), y.to(device), u.to(device), domain.to(device)\n",
    "        domain = domain[:, 0]\n",
    "        is_source = (domain < 1.0 / 8).to(torch.float)\n",
    "\n",
    "        x_align, features = encoder(x, u)\n",
    "        predictions = classifier(features)\n",
    "\n",
    "        g = torch.argmax(predictions.detach(), dim=1)\n",
    "        metrics_domain = hit_domain, cnt_domain\n",
    "        metrics_source = acc_source, cnt_source, hit_source\n",
    "        metrics_target = acc_target, cnt_target, hit_target\n",
    "        hit_domain, cnt_domain, acc_source, acc_target, cnt_source, cnt_target, hit_source, hit_target = acc_update_mnist(y, g, domain, is_source, metrics_domain, metrics_source, metrics_target)\n",
    "\n",
    "    print(f' \\t Val Source Acc:{acc_source:0.3f} ({hit_source}/{cnt_source}) \\t Val Target Acc:{acc_target:0.3f} ({hit_target}/{cnt_target})')\n",
    "\n",
    "def generate_result_table(encoder, classifier, discriminator):\n",
    "    field_names = [\"Accuracy\"] + [\"Source\"] + [f\"Target #{i}\" for i in range(1, 8)]\n",
    "\n",
    "    hit = np.zeros((10, 8))\n",
    "    cnt = np.zeros((10, 8))\n",
    "\n",
    "    for batch_idx, (x, y, u, domain) in enumerate(test_dataloader):\n",
    "        x, y, u, domain = x.to(device), y.to(device), u.to(device), domain.to(device)\n",
    "        domain = domain[:, 0]\n",
    "        is_source = (domain < 1.0 / 8).to(torch.float)\n",
    "\n",
    "        x_align, features = encoder(x, u)\n",
    "        predictions = classifier(features)\n",
    "\n",
    "        g = torch.argmax(predictions.detach(), dim=1)\n",
    "\n",
    "        Y = to_np(y)\n",
    "        G = to_np(g)\n",
    "        T = to_np(u)[:, 0]\n",
    "        T = (T * 8).astype(np.int32)\n",
    "        T[T >= 8] = 7\n",
    "\n",
    "        for label, pred, domain in zip(Y, G, T):\n",
    "            hit[label, domain] += int(label == pred)\n",
    "            cnt[label, domain] += 1\n",
    "\n",
    "    print('Accuracy Source Target #1 #2 #3 #4 #5 #6 #7')\n",
    "    for c in range(10):\n",
    "        print(f'Class {c}' + str(list(np.round(100 * hit[c] / cnt[c], decimals=1))))\n",
    "    print(f'Total' + str(list(np.round(100 * hit.sum(0) / cnt.sum(0), decimals=1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kidd\\OneDrive - Temasek Polytechnic (1)\\Year 3 Sem 2\\Internship\\Deep Learning with PyTorch\\CIDA\\rotated-mnist-cida\\modules\\modules.py:96: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.\n",
      "The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n",
      "L, _ = torch.symeig(A, upper=upper)\n",
      "should be replaced with\n",
      "L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n",
      "and\n",
      "L, V = torch.symeig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2499.)\n",
      "  _, evs = torch.symeig(A, eigenvectors=True)\n",
      "c:\\Users\\Kidd\\miniconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\functional.py:4066: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "c:\\Users\\Kidd\\miniconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      " \t D Loss:0.097 \t E_gan Loss:0.083 \t E_pred Loss:0.758\n",
      " \t Source Acc:0.763 (5683.0/7450) \t Target Acc:0.409 (21508.0/52550)\n",
      " \t Val Source Acc:0.920 (6770.0/7356) \t Val Target Acc:0.483 (25446.0/52644)\n",
      "Epoch: 2 \n",
      " \t D Loss:0.062 \t E_gan Loss:0.058 \t E_pred Loss:0.316\n",
      " \t Source Acc:0.898 (6604.0/7357) \t Target Acc:0.446 (23483.0/52643)\n",
      " \t Val Source Acc:0.936 (6888.0/7360) \t Val Target Acc:0.437 (22945.0/52640)\n",
      "Epoch: 3 \n",
      " \t D Loss:0.045 \t E_gan Loss:0.043 \t E_pred Loss:0.198\n",
      " \t Source Acc:0.942 (6992.0/7424) \t Target Acc:0.466 (24501.0/52576)\n",
      " \t Val Source Acc:0.953 (7052.0/7400) \t Val Target Acc:0.512 (26885.0/52600)\n",
      "Epoch: 4 \n",
      " \t D Loss:0.042 \t E_gan Loss:0.041 \t E_pred Loss:0.175\n",
      " \t Source Acc:0.948 (7102.0/7490) \t Target Acc:0.496 (26061.0/52510)\n",
      " \t Val Source Acc:0.946 (7048.0/7453) \t Val Target Acc:0.486 (25490.0/52547)\n",
      "Epoch: 5 \n",
      " \t D Loss:0.039 \t E_gan Loss:0.038 \t E_pred Loss:0.148\n",
      " \t Source Acc:0.952 (7122.0/7483) \t Target Acc:0.501 (26302.0/52517)\n",
      " \t Val Source Acc:0.965 (7316.0/7578) \t Val Target Acc:0.494 (25869.0/52422)\n",
      "Epoch: 6 \n",
      " \t D Loss:0.038 \t E_gan Loss:0.037 \t E_pred Loss:0.136\n",
      " \t Source Acc:0.961 (7273.0/7570) \t Target Acc:0.504 (26446.0/52430)\n",
      " \t Val Source Acc:0.971 (7236.0/7455) \t Val Target Acc:0.519 (27238.0/52545)\n",
      "Epoch: 7 \n",
      " \t D Loss:0.036 \t E_gan Loss:0.035 \t E_pred Loss:0.135\n",
      " \t Source Acc:0.958 (7179.0/7490) \t Target Acc:0.504 (26431.0/52510)\n",
      " \t Val Source Acc:0.968 (7100.0/7337) \t Val Target Acc:0.521 (27381.0/52663)\n",
      "Epoch: 8 \n",
      " \t D Loss:0.033 \t E_gan Loss:0.033 \t E_pred Loss:0.121\n",
      " \t Source Acc:0.964 (7253.0/7525) \t Target Acc:0.512 (26928.0/52475)\n",
      " \t Val Source Acc:0.970 (7353.0/7580) \t Val Target Acc:0.530 (27776.0/52420)\n",
      "Epoch: 9 \n",
      " \t D Loss:0.030 \t E_gan Loss:0.029 \t E_pred Loss:0.107\n",
      " \t Source Acc:0.968 (7248.0/7491) \t Target Acc:0.514 (27002.0/52509)\n",
      " \t Val Source Acc:0.968 (7181.0/7421) \t Val Target Acc:0.507 (26595.0/52579)\n",
      "Epoch: 10 \n",
      " \t D Loss:0.032 \t E_gan Loss:0.031 \t E_pred Loss:0.104\n",
      " \t Source Acc:0.967 (7285.0/7531) \t Target Acc:0.506 (26594.0/52469)\n",
      " \t Val Source Acc:0.976 (7377.0/7556) \t Val Target Acc:0.563 (29549.0/52444)\n",
      "Epoch: 11 \n",
      " \t D Loss:0.033 \t E_gan Loss:0.032 \t E_pred Loss:0.107\n",
      " \t Source Acc:0.970 (7281.0/7508) \t Target Acc:0.530 (27791.0/52492)\n",
      " \t Val Source Acc:0.968 (7357.0/7597) \t Val Target Acc:0.543 (28495.0/52403)\n",
      "Epoch: 12 \n",
      " \t D Loss:0.031 \t E_gan Loss:0.030 \t E_pred Loss:0.089\n",
      " \t Source Acc:0.973 (7276.0/7477) \t Target Acc:0.538 (28272.0/52523)\n",
      " \t Val Source Acc:0.972 (7224.0/7429) \t Val Target Acc:0.547 (28671.0/52571)\n",
      "Epoch: 13 \n",
      " \t D Loss:0.030 \t E_gan Loss:0.030 \t E_pred Loss:0.095\n",
      " \t Source Acc:0.970 (7232.0/7459) \t Target Acc:0.549 (28765.0/52541)\n",
      " \t Val Source Acc:0.974 (7351.0/7551) \t Val Target Acc:0.560 (29352.0/52449)\n",
      "Epoch: 14 \n",
      " \t D Loss:0.030 \t E_gan Loss:0.030 \t E_pred Loss:0.093\n",
      " \t Source Acc:0.971 (7235.0/7450) \t Target Acc:0.532 (27878.0/52550)\n",
      " \t Val Source Acc:0.980 (7416.0/7568) \t Val Target Acc:0.572 (29991.0/52432)\n",
      "Epoch: 15 \n",
      " \t D Loss:0.028 \t E_gan Loss:0.028 \t E_pred Loss:0.085\n",
      " \t Source Acc:0.975 (7289.0/7478) \t Target Acc:0.559 (29290.0/52522)\n",
      " \t Val Source Acc:0.974 (7259.0/7449) \t Val Target Acc:0.542 (28503.0/52551)\n",
      "Epoch: 16 \n",
      " \t D Loss:0.030 \t E_gan Loss:0.029 \t E_pred Loss:0.088\n",
      " \t Source Acc:0.976 (7376.0/7561) \t Target Acc:0.560 (29274.0/52439)\n",
      " \t Val Source Acc:0.982 (7424.0/7559) \t Val Target Acc:0.551 (28907.0/52441)\n",
      "Epoch: 17 \n",
      " \t D Loss:0.030 \t E_gan Loss:0.030 \t E_pred Loss:0.083\n",
      " \t Source Acc:0.974 (7468.0/7664) \t Target Acc:0.566 (29648.0/52336)\n",
      " \t Val Source Acc:0.975 (7181.0/7365) \t Val Target Acc:0.560 (29391.0/52635)\n",
      "Epoch: 18 \n",
      " \t D Loss:0.028 \t E_gan Loss:0.028 \t E_pred Loss:0.074\n",
      " \t Source Acc:0.977 (7323.0/7499) \t Target Acc:0.542 (28504.0/52501)\n",
      " \t Val Source Acc:0.971 (7390.0/7607) \t Val Target Acc:0.490 (25659.0/52393)\n",
      "Epoch: 19 \n",
      " \t D Loss:0.027 \t E_gan Loss:0.027 \t E_pred Loss:0.070\n",
      " \t Source Acc:0.979 (7278.0/7437) \t Target Acc:0.530 (27813.0/52563)\n",
      " \t Val Source Acc:0.971 (7287.0/7503) \t Val Target Acc:0.532 (27872.0/52497)\n",
      "Epoch: 20 \n",
      " \t D Loss:0.027 \t E_gan Loss:0.027 \t E_pred Loss:0.073\n",
      " \t Source Acc:0.979 (7332.0/7486) \t Target Acc:0.532 (27981.0/52514)\n",
      " \t Val Source Acc:0.975 (7377.0/7564) \t Val Target Acc:0.540 (28339.0/52436)\n",
      "Epoch: 21 \n",
      " \t D Loss:0.026 \t E_gan Loss:0.026 \t E_pred Loss:0.074\n",
      " \t Source Acc:0.979 (7413.0/7572) \t Target Acc:0.552 (29002.0/52428)\n",
      " \t Val Source Acc:0.981 (7297.0/7440) \t Val Target Acc:0.579 (30401.0/52560)\n",
      "Epoch: 22 \n",
      " \t D Loss:0.026 \t E_gan Loss:0.025 \t E_pred Loss:0.071\n",
      " \t Source Acc:0.981 (7444.0/7591) \t Target Acc:0.542 (28355.0/52409)\n",
      " \t Val Source Acc:0.986 (7416.0/7521) \t Val Target Acc:0.564 (29548.0/52479)\n",
      "Epoch: 23 \n",
      " \t D Loss:0.023 \t E_gan Loss:0.023 \t E_pred Loss:0.068\n",
      " \t Source Acc:0.979 (7169.0/7321) \t Target Acc:0.563 (29550.0/52679)\n",
      " \t Val Source Acc:0.982 (7451.0/7586) \t Val Target Acc:0.534 (28055.0/52414)\n",
      "Epoch: 24 \n",
      " \t D Loss:0.023 \t E_gan Loss:0.023 \t E_pred Loss:0.058\n",
      " \t Source Acc:0.983 (7282.0/7411) \t Target Acc:0.528 (27767.0/52589)\n",
      " \t Val Source Acc:0.982 (7264.0/7398) \t Val Target Acc:0.571 (29991.0/52602)\n",
      "Epoch: 25 \n",
      " \t D Loss:0.025 \t E_gan Loss:0.025 \t E_pred Loss:0.069\n",
      " \t Source Acc:0.980 (7373.0/7527) \t Target Acc:0.567 (29782.0/52473)\n",
      " \t Val Source Acc:0.981 (7449.0/7590) \t Val Target Acc:0.572 (29953.0/52410)\n",
      "Epoch: 26 \n",
      " \t D Loss:0.024 \t E_gan Loss:0.024 \t E_pred Loss:0.059\n",
      " \t Source Acc:0.983 (7414.0/7543) \t Target Acc:0.611 (32000.0/52457)\n",
      " \t Val Source Acc:0.981 (7362.0/7504) \t Val Target Acc:0.606 (31781.0/52496)\n",
      "Epoch: 27 \n",
      " \t D Loss:0.022 \t E_gan Loss:0.022 \t E_pred Loss:0.057\n",
      " \t Source Acc:0.983 (7311.0/7438) \t Target Acc:0.599 (31524.0/52562)\n",
      " \t Val Source Acc:0.981 (7440.0/7581) \t Val Target Acc:0.594 (31182.0/52419)\n",
      "Epoch: 28 \n",
      " \t D Loss:0.023 \t E_gan Loss:0.023 \t E_pred Loss:0.065\n",
      " \t Source Acc:0.981 (7516.0/7658) \t Target Acc:0.592 (30947.0/52342)\n",
      " \t Val Source Acc:0.981 (7442.0/7587) \t Val Target Acc:0.562 (29428.0/52413)\n",
      "Epoch: 29 \n",
      " \t D Loss:0.021 \t E_gan Loss:0.021 \t E_pred Loss:0.063\n",
      " \t Source Acc:0.981 (7273.0/7411) \t Target Acc:0.563 (29618.0/52589)\n",
      " \t Val Source Acc:0.986 (7270.0/7375) \t Val Target Acc:0.566 (29743.0/52625)\n",
      "Epoch: 30 \n",
      " \t D Loss:0.021 \t E_gan Loss:0.021 \t E_pred Loss:0.054\n",
      " \t Source Acc:0.983 (7348.0/7472) \t Target Acc:0.582 (30628.0/52528)\n",
      " \t Val Source Acc:0.982 (7367.0/7499) \t Val Target Acc:0.605 (31749.0/52501)\n",
      "Epoch: 31 \n",
      " \t D Loss:0.020 \t E_gan Loss:0.020 \t E_pred Loss:0.054\n",
      " \t Source Acc:0.984 (7280.0/7396) \t Target Acc:0.598 (31461.0/52604)\n",
      " \t Val Source Acc:0.984 (7472.0/7596) \t Val Target Acc:0.600 (31468.0/52404)\n",
      "Epoch: 32 \n",
      " \t D Loss:0.021 \t E_gan Loss:0.021 \t E_pred Loss:0.054\n",
      " \t Source Acc:0.985 (7366.0/7481) \t Target Acc:0.592 (31065.0/52519)\n",
      " \t Val Source Acc:0.983 (7371.0/7502) \t Val Target Acc:0.594 (31224.0/52498)\n",
      "Epoch: 33 \n",
      " \t D Loss:0.020 \t E_gan Loss:0.019 \t E_pred Loss:0.053\n",
      " \t Source Acc:0.984 (7387.0/7507) \t Target Acc:0.584 (30610.0/52493)\n",
      " \t Val Source Acc:0.986 (7538.0/7642) \t Val Target Acc:0.584 (30572.0/52358)\n",
      "Epoch: 34 \n",
      " \t D Loss:0.020 \t E_gan Loss:0.020 \t E_pred Loss:0.061\n",
      " \t Source Acc:0.981 (7317.0/7455) \t Target Acc:0.560 (29351.0/52545)\n",
      " \t Val Source Acc:0.983 (7351.0/7476) \t Val Target Acc:0.544 (28606.0/52524)\n",
      "Epoch: 35 \n",
      " \t D Loss:0.027 \t E_gan Loss:0.027 \t E_pred Loss:0.069\n",
      " \t Source Acc:0.980 (7294.0/7445) \t Target Acc:0.536 (28134.0/52555)\n",
      " \t Val Source Acc:0.980 (7434.0/7588) \t Val Target Acc:0.545 (28548.0/52412)\n",
      "Epoch: 36 \n",
      " \t D Loss:0.021 \t E_gan Loss:0.021 \t E_pred Loss:0.055\n",
      " \t Source Acc:0.983 (7403.0/7528) \t Target Acc:0.544 (28506.0/52472)\n",
      " \t Val Source Acc:0.989 (7438.0/7523) \t Val Target Acc:0.568 (29798.0/52477)\n",
      "Epoch: 37 \n",
      " \t D Loss:0.020 \t E_gan Loss:0.020 \t E_pred Loss:0.060\n",
      " \t Source Acc:0.984 (7374.0/7496) \t Target Acc:0.569 (29816.0/52504)\n",
      " \t Val Source Acc:0.985 (7429.0/7542) \t Val Target Acc:0.555 (29103.0/52458)\n",
      "Epoch: 38 \n",
      " \t D Loss:0.020 \t E_gan Loss:0.020 \t E_pred Loss:0.061\n",
      " \t Source Acc:0.981 (7236.0/7374) \t Target Acc:0.597 (31481.0/52626)\n",
      " \t Val Source Acc:0.983 (7330.0/7453) \t Val Target Acc:0.586 (30789.0/52547)\n",
      "Epoch: 39 \n",
      " \t D Loss:0.017 \t E_gan Loss:0.017 \t E_pred Loss:0.048\n",
      " \t Source Acc:0.984 (7380.0/7503) \t Target Acc:0.601 (31552.0/52497)\n",
      " \t Val Source Acc:0.987 (7391.0/7486) \t Val Target Acc:0.590 (30980.0/52514)\n",
      "Epoch: 40 \n",
      " \t D Loss:0.018 \t E_gan Loss:0.018 \t E_pred Loss:0.046\n",
      " \t Source Acc:0.986 (7400.0/7502) \t Target Acc:0.588 (30772.0/52498)\n",
      " \t Val Source Acc:0.986 (7406.0/7508) \t Val Target Acc:0.594 (31175.0/52492)\n",
      "Epoch: 41 \n",
      " \t D Loss:0.018 \t E_gan Loss:0.017 \t E_pred Loss:0.049\n",
      " \t Source Acc:0.985 (7374.0/7483) \t Target Acc:0.573 (30073.0/52517)\n",
      " \t Val Source Acc:0.987 (7479.0/7577) \t Val Target Acc:0.555 (29110.0/52423)\n",
      "Epoch: 42 \n",
      " \t D Loss:0.017 \t E_gan Loss:0.016 \t E_pred Loss:0.042\n",
      " \t Source Acc:0.987 (7206.0/7301) \t Target Acc:0.569 (29917.0/52699)\n",
      " \t Val Source Acc:0.987 (7341.0/7438) \t Val Target Acc:0.575 (30205.0/52562)\n",
      "Epoch: 43 \n",
      " \t D Loss:0.016 \t E_gan Loss:0.016 \t E_pred Loss:0.043\n",
      " \t Source Acc:0.988 (7349.0/7437) \t Target Acc:0.568 (29805.0/52563)\n",
      " \t Val Source Acc:0.983 (7382.0/7508) \t Val Target Acc:0.572 (30035.0/52492)\n",
      "Epoch: 44 \n",
      " \t D Loss:0.017 \t E_gan Loss:0.016 \t E_pred Loss:0.050\n",
      " \t Source Acc:0.985 (7453.0/7566) \t Target Acc:0.590 (30949.0/52434)\n",
      " \t Val Source Acc:0.989 (7470.0/7556) \t Val Target Acc:0.589 (30846.0/52444)\n",
      "Epoch: 45 \n",
      " \t D Loss:0.016 \t E_gan Loss:0.016 \t E_pred Loss:0.043\n",
      " \t Source Acc:0.985 (7474.0/7591) \t Target Acc:0.600 (31378.0/52409)\n",
      " \t Val Source Acc:0.983 (7292.0/7421) \t Val Target Acc:0.585 (30797.0/52579)\n",
      "Epoch: 46 \n",
      " \t D Loss:0.016 \t E_gan Loss:0.016 \t E_pred Loss:0.044\n",
      " \t Source Acc:0.986 (7411.0/7514) \t Target Acc:0.590 (31015.0/52486)\n",
      " \t Val Source Acc:0.988 (7381.0/7473) \t Val Target Acc:0.581 (30525.0/52527)\n",
      "Epoch: 47 \n",
      " \t D Loss:0.015 \t E_gan Loss:0.015 \t E_pred Loss:0.047\n",
      " \t Source Acc:0.987 (7397.0/7495) \t Target Acc:0.579 (30318.0/52505)\n",
      " \t Val Source Acc:0.988 (7420.0/7507) \t Val Target Acc:0.572 (30027.0/52493)\n",
      "Epoch: 48 \n",
      " \t D Loss:0.015 \t E_gan Loss:0.015 \t E_pred Loss:0.054\n",
      " \t Source Acc:0.985 (7304.0/7415) \t Target Acc:0.579 (30417.0/52585)\n",
      " \t Val Source Acc:0.987 (7308.0/7402) \t Val Target Acc:0.595 (31208.0/52598)\n",
      "Epoch: 49 \n",
      " \t D Loss:0.014 \t E_gan Loss:0.014 \t E_pred Loss:0.041\n",
      " \t Source Acc:0.988 (7378.0/7465) \t Target Acc:0.584 (30668.0/52535)\n",
      " \t Val Source Acc:0.989 (7234.0/7312) \t Val Target Acc:0.581 (30647.0/52688)\n",
      "Epoch: 50 \n",
      " \t D Loss:0.013 \t E_gan Loss:0.013 \t E_pred Loss:0.045\n",
      " \t Source Acc:0.986 (7286.0/7390) \t Target Acc:0.561 (29529.0/52610)\n",
      " \t Val Source Acc:0.984 (7498.0/7622) \t Val Target Acc:0.550 (28761.0/52378)\n"
     ]
    }
   ],
   "source": [
    "encoder, classifier, discriminator = train(encoder, classifier, discriminator, epochs, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Source Target #1 #2 #3 #4 #5 #6 #7\n",
      "Class 0[98.8, 96.8, 92.6, 85.2, 90.9, 93.9, 92.5, 81.5]\n",
      "Class 1[99.4, 99.2, 95.8, 92.2, 87.0, 71.1, 9.6, 8.0]\n",
      "Class 2[96.8, 77.2, 40.3, 70.6, 95.2, 85.1, 49.9, 68.9]\n",
      "Class 3[97.0, 83.1, 59.4, 55.8, 77.1, 34.2, 6.6, 8.8]\n",
      "Class 4[98.9, 91.2, 46.3, 31.3, 60.3, 40.2, 19.3, 33.6]\n",
      "Class 5[98.7, 87.8, 63.5, 71.2, 89.1, 52.8, 23.9, 58.2]\n",
      "Class 6[98.4, 80.5, 20.6, 41.8, 73.9, 61.5, 47.5, 71.4]\n",
      "Class 7[96.9, 82.8, 41.8, 12.9, 26.2, 48.7, 29.4, 30.6]\n",
      "Class 8[98.1, 87.8, 64.4, 53.7, 65.3, 39.7, 6.2, 18.5]\n",
      "Class 9[98.3, 91.3, 44.6, 3.3, 7.0, 8.0, 6.7, 17.8]\n",
      "Total[98.1, 87.7, 57.4, 52.4, 66.7, 54.3, 29.0, 38.8]\n"
     ]
    }
   ],
   "source": [
    "generate_result_table(encoder, classifier, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbbf5940fb39bd3f383e6d88b474dca1e13ffd10acf1bdc8d3732bf030dd4f5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
